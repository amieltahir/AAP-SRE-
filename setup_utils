trsadmin@trscore:~/aap_lab/2.4_rhel9$ wget --no-check-certificate --content-disposition "https://access.cdn.redhat.com/content/origin/files/sha256/64/64d6f7661271faeae6491ab946d79d87ed87f2446490211840448eaf06929ee4/ansible-automation-platform-setup-bundle-2.4-13.4-x86_64.tar.gz?user=282b9163b4bb0674d4f1f3a5219907f2&_auth_=1761287879_6f314ba54db1a0da3c880cf4bbbfaaa7"

tar -xvf  ansible-automation-platform-setup-bundle-2.4-13.4-x86_64.tar.gz


### #################install terraform ##########

sudo apt-get update -y
sudo apt-get install -y gnupg software-properties-common curl
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt-get update -y
sudo apt-get install -y terraform
####################################

### #################install aws-cli ##########

sudo apt-get install -y unzip
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
aws --version
aws configure

stored in 
~/.aws/credentials
~/.aws/config

export AWS_ACCESS_KEY_ID="YOUR_ACCESS_KEY"
export AWS_SECRET_ACCESS_KEY="YOUR_SECRET_KEY"
export AWS_DEFAULT_REGION="us-east-1"

add in end of file 
 vi ~/.bashrc
 source ~/.bashrc
aws sts get-caller-identity 


####################################

after we prepared files in terraform now we execute 

 terraform init
Initializing the backend...
Initializing provider plugins...
- Finding latest version of hashicorp/aws...
- Finding latest version of hashicorp/local...
- Finding latest version of hashicorp/tls...
- Installing hashicorp/aws v6.18.0...
- Installed hashicorp/aws v6.18.0 (signed by HashiCorp)
- Installing hashicorp/local v2.5.3...
- Installed hashicorp/local v2.5.3 (signed by HashiCorp)
- Installing hashicorp/tls v4.1.0...
- Installed hashicorp/tls v4.1.0 (signed by HashiCorp)
Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.

----

terraform plan
terraform apply -auto-approve

---

curl https://checkip.amazonaws.com


aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$(terraform output -raw vpc_id)" --query "RouteTables[].Routes"
[
    [
        {
            "DestinationCidrBlock": "10.10.0.0/16",
            "GatewayId": "local",
            "Origin": "CreateRouteTable",
            "State": "active"
        },
        {
            "DestinationCidrBlock": "0.0.0.0/0",
            "GatewayId": "igw-0226e1e865f07cacb",
            "Origin": "CreateRoute",
            "State": "active"
        }
    ],
    [
        {
            "DestinationCidrBlock": "10.10.0.0/16",
            "GatewayId": "local",
            "Origin": "CreateRouteTable",
            "State": "active"
        }
    ]
]

-------------

now im able to ping them all from my control machine

ansible all -i inventory.yml -m ping

exec01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.9"
    },
    "changed": false,
    "ping": "pong"
}
db01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.9"
    },
    "changed": false,
    "ping": "pong"
}
controller01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.9"
    },
    "changed": false,
    "ping": "pong"
}
hub01 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3.9"
    },
    "changed": false,
    "ping": "pong"
}


-----------------

we already have SSH access with key.pem, we can now set up true passwordless SSH between all four instances, so that the controller (or any node) can connect to the others without needing key.pem each time. This is useful for Ansible to operate smoothly across all nodes.

we can automate this fully. Here’s a clean Ansible playbook that:
Ensures an SSH key exists on the controller.
Copies that key to the hub, exec, and db nodes for passwordless SSH.
playbook name -> setup_ssh_controller_and_distribute.yml

How it works:

First play: Runs on the controller, ensures ~/.ssh/id_rsa exists (creates one if missing).
Second play: Runs on all other nodes (hub, exec, db) and copies the controller’s public key to ~/.ssh/authorized_keys.
After this, controller can SSH to all nodes without passwords or key files.

Running the playbook:
ansible-playbook -i inventory.yml setup_ssh.yml --private-key=key.pem
--private-key=key.pem lets Ansible initially connect using your existing key.
After this playbook runs, future Ansible runs won’t need key.pem for these hosts.

How this works:
controller01 generates the key if missing.
slurp reads the controller’s public key and encodes it in base64.
All other nodes (hub, exec, db) get the decoded key via authorized_key.

## test 
ansible all -i inventory.yml -m ping -u ec2-user
controller01 | SUCCESS => { "changed": false, "ping": "pong" }
hub01        | SUCCESS => { "changed": false, "ping": "pong" }
exec01       | SUCCESS => { "changed": false, "ping": "pong" }
db01         | SUCCESS => { "changed": false, "ping": "pong" }
#test 2
ssh controller01
ssh hub01
ssh exec01
ssh db01
#test3 
ansible-playbook -i inventory.yml test_connectivity.yml

PLAY [Test inter-node connectivity] *****************************************************************************************************************************************

TASK [Ping each node] *******************************************************************************************************************************************************
ok: [controller01]
ok: [exec01]
ok: [hub01]
ok: [db01]

PLAY RECAP ******************************************************************************************************************************************************************
controller01               : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
db01                       : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
exec01                     : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
hub01                      : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0

#### to be able to run local ssh setup  -> setup_ssh_config_local.yml

cp ~/aap_lab/terraform/key.pem ~/aap_lab/ansible/key.pem
chmod 600 ~/aap_lab/ansible/key.pem

---------------------------

sudo checks

ssh  ec2-user@controller01 "sudo -n true && echo OK || echo FAIL"
ssh  ec2-user@exec01 "sudo -n true && echo OK || echo FAIL"
ssh  ec2-user@hub01 "sudo -n true && echo OK || echo FAIL"
ssh  ec2-user@db01 "sudo -n true && echo OK || echo FAIL"


----
checking port 80 and 443 are accessible

aws ec2 describe-security-groups \
    --group-ids sg-0ec2870ef0ce830cd \
    --query 'SecurityGroups[].IpPermissions[?FromPort!=`null` && (FromPort==`80` || FromPort==`443`)]' \
    --output json
[
    [
        {
            "IpProtocol": "tcp",
            "FromPort": 80,
            "ToPort": 80,
            "UserIdGroupPairs": [],
            "IpRanges": [
                {
                    "CidrIp": "0.0.0.0/0"
                }
            ],
            "Ipv6Ranges": [],
            "PrefixListIds": []
        },
        {
            "IpProtocol": "tcp",
            "FromPort": 443,
            "ToPort": 443,
            "UserIdGroupPairs": [],
            "IpRanges": [
                {
                    "CidrIp": "0.0.0.0/0"
                }
            ],
            "Ipv6Ranges": [],
            "PrefixListIds": []
        }
    ]
]
----

ansible-galaxy collection install community.crypto --upgrade

sudo apt install -y python3-pip
sudo apt install -y python-is-python3
python --version  # Should show Python 3.13.3

sudo apt install -y build-essential libssl-dev zlib1g-dev libbz2-dev \
libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev \
libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev git

curl https://pyenv.run | bash

export PATH="$HOME/.pyenv/bin:$PATH"
eval "$(pyenv init --path)"
eval "$(pyenv virtualenv-init -)"

pip install --upgrade pip
pip install ansible-core==2.14
ansible --version
_----------------

ansible -i ~/aap_lab/ansible/inventory.yml all -m file -a "path=/etc/pki/tls/certs state=absent" -b
ansible -i ~/aap_lab/ansible/inventory.yml all -m file -a "path=/etc/pki/tls/private state=absent" -b

# Kill all SSH control processes and clean sockets
pkill -f 'ssh.*ControlMaster'
rm -f ~/.ssh/ansible-* ~/.ssh/ansible_* ~/.ssh/*@*:*


